<!DOCTYPE html>  
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <title>summary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
#wrapper{font-family:Palatino,Georgia,"Times New Roman",serif;font-weight:normal;font-size:0.8764em;line-height:1.5em;margin:0;font-size:14px}#wrapper p{margin:1.3125em 0;font-size:1.1429em;line-height:1.6125em}#wrapper ul{list-style:square}#wrapper ol{list-style:decimal}#wrapper ul,#wrapper ol{padding-left:0;list-style-position:outside}#wrapper ul li,#wrapper ol li{font-size:110%;line-height:1.525em}#wrapper ul li li,#wrapper ol li li{font-size:100%}#wrapper ul li p,#wrapper ol li p{font-size:100%;margin-left:1em}#wrapper ul ul,#wrapper ul ol,#wrapper ol ul,#wrapper ol ol{margin-bottom:.4em;padding-left:30px}#wrapper ul ul,#wrapper ol ul{list-style:circle}#wrapper strong{font-weight:bold}#wrapper em{font-style:italic}#wrapper h1{margin:0.6563em 0;font-size:2.8em;line-height:1.6563em}#wrapper h2{margin:0.875em 0;font-size:1.7143em;line-height:0.875em}#wrapper h3{margin:1em 0;font-size:1.5em;line-height:1em}#wrapper h4{margin:1.1667em 0;font-size:1.2857em;line-height:1.1667em}#wrapper h5{margin:1.3125em 0;font-size:1.1429em;line-height:1.3125em}#wrapper h6{margin:1.5em 0;font-size:1em;line-height:1.5em}#wrapper h1,#wrapper h2,#wrapper h3,#wrapper h4,#wrapper h5,#wrapper h6{font-weight:bold}#wrapper h1 a,#wrapper h2 a,#wrapper h3 a,#wrapper h4 a,#wrapper h5 a,#wrapper h6 a{color:#514d3f;border-bottom:dotted 1px #a7a099}#wrapper h1 a:hover,#wrapper h2 a:hover,#wrapper h3 a:hover,#wrapper h4 a:hover,#wrapper h5 a:hover,#wrapper h6 a:hover{color:#13120f}#wrapper h1,#wrapper h2{padding-bottom:.4em;padding-top:.25em;font-family:"Hoefler Text",Georgia,serif;line-height:1.5em;color:#514d3f;text-shadow:1px 0px 0px #a7a099,0 1px 0 #fff}#wrapper h1{text-align:center}#wrapper h1:before{content:"\0360";position:relative;left:-30px;top:.4em;color:#837e70;text-shadow:1px 0px 0px #a7a099,0 1px 0 #fff}#wrapper h1:after{content:"\0360";position:relative;right:-30px;top:.4em;color:#837e70;text-shadow:1px 0px 0px #a7a099,0 1px 0 #fff}#wrapper h2{box-shadow:0 -1px 0 #fff,0 -2px 0 #837e70;margin-top:1em;padding-bottom:0}#wrapper,#wrapper p,#wrapper td,#wrapper div{color:#45462f;word-wrap:break-word;-webkit-font-smoothing:antialiased}#wrapper a{color:#00857a;text-decoration:none;-webkit-transition:color 0.1s ease-in-out;-moz-transition:color 0.1s ease-in-out;-o-transition:color 0.1s ease-in-out;-ms-transition:color 0.1s ease-in-out;transition:color 0.1s ease-in-out}#wrapper a:hover{color:#35b8ad}#wrapper strong{font-weight:600;color:#605b4b}#wrapper .footnote{font-size:.8em;vertical-align:super;color:#0d6ea1}#wrapper img{max-width:100%;height:auto;border:solid 2px rgba(56,40,18,0.25)}#wrapper dt{font-weight:bold}#wrapper dd{margin-bottom:1em;text-indent:1em}#wrapper blockquote{margin:14px 40px}#wrapper code{font-family:courier, monospace;background:#f2ede8;border:solid 1px #a7a099;font-size:.95em}#wrapper pre{background:#f2ede8;padding:4px}#wrapper pre code{font-size:1em;margin:0;padding:4px;border:none;background:none}#wrapper hr{border-color:#f2ede8;height:0;background-color:#a7a099}@media print{body{overflow:auto;background:#fff;color:#000 !important}img,pre,blockquote,table,figure{page-break-inside:avoid}#wrapper{background:#fff;position:relative;color:#000;text-indent:0px;padding:1in;font-size:85%}}@media screen{body{height:100%;background:#f0edeb}#wrapper{color:#514d3f;overflow:auto;text-indent:0px;padding:25px}#wrapper ::selection{background:rgba(157,193,200,0.5)}#wrapper h1::selection,#wrapper h2::selection,#wrapper h3::selection,#wrapper h4::selection,#wrapper h5::selection,#wrapper h6::selection{background-color:rgba(133,201,232,0.3)}#wrapper code::selection{background-color:rgba(0,0,0,0.7);color:#eee}#wrapper code span::selection{background-color:rgba(0,0,0,0.7) !important;color:#eee !important}#wrapper a::selection{background-color:rgba(255,230,102,0.2)}#wrapper td::selection,#wrapper th::selection,#wrapper caption::selection{background-color:rgba(180,237,95,0.5)}.inverted{background:#39362f}.inverted #wrapper{background:#39362f}.inverted #wrapper a::selection{background-color:rgba(255,230,102,0.6)}.inverted #wrapper p,.inverted #wrapper td,.inverted #wrapper li,.inverted #wrapper h1,.inverted #wrapper h2,.inverted #wrapper h3,.inverted #wrapper h4,.inverted #wrapper h5,.inverted #wrapper h6,.inverted #wrapper pre,.inverted #wrapper code,.inverted #wrapper th,.inverted #wrapper hr,.inverted #wrapper strong,.inverted #wrapper em,.inverted #wrapper .math,.inverted #wrapper dd,.inverted #wrapper dt{color:#f0ede3;border-color:#423d2f;background:#39362f}.inverted #wrapper hr{border-color:rgba(52,52,52,0.3)}.inverted #wrapper a{color:#fff;text-decoration:underline;border-bottom:none}.inverted #wrapper .popup li,.inverted #wrapper .popup ul,.inverted #wrapper .popup strong{background:none}}#wrapper li>p:first-child{margin:0}#wrapper li p{margin:.5em 0}#wrapper caption,#wrapper col,#wrapper colgroup,#wrapper table,#wrapper tbody,#wrapper td,#wrapper tfoot,#wrapper th,#wrapper thead,#wrapper tr{border-spacing:0}#wrapper caption{display:table-caption;font-weight:bold}#wrapper col{display:table-column}#wrapper colgroup{display:table-column-group}#wrapper tbody{display:table-row-group}#wrapper tfoot{display:table-footer-group}#wrapper thead{display:table-header-group}#wrapper td,#wrapper th{display:table-cell}#wrapper th{font-weight:bold}#wrapper tr{display:table-row}#wrapper table{display:table;table-layout:fixed;border-collapse:collapse;empty-cells:hide;margin:0 0 24px 0;padding:0;border:0;margin-top:-1px;margin-bottom:23px;border:1px solid rgba(124,119,112,0.5)}#wrapper table th,#wrapper table td{padding:0 1em;font-size:1.1em;line-height:23px}#wrapper table tbody{background-color:rgba(124,119,112,0.05)}#wrapper table thead,#wrapper table tfoot{background-color:rgba(124,119,112,0.15)}#wrapper table tr:nth-child(odd){background-color:rgba(227,217,205,0.06)}#wrapper table tr:nth-child(even),#wrapper table td:nth-child(even){background-color:rgba(124,119,112,0.06)}#wrapper table thead,#wrapper table tfoot{border:1px solid rgba(124,119,112,0.5);border-bottom:1px solid rgba(124,119,112,0.2)}#wrapper table thead tr th:last-child{border-right:1px solid rgba(124,119,112,0.5)}#wrapper figure{position:relative;display:inline-block;margin-bottom:2em}#wrapper figure:hover{cursor:pointer}#wrapper figcaption{text-align:center;position:absolute;background:transparent;width:100%;left:0;bottom:-20px;color:#a7a099}#wrapper .poetry pre{font-family:Georgia,Garamond,serif !important;font-style:italic;font-size:110% !important;line-height:1.6em;display:block;margin-left:1em}#wrapper .poetry pre code{font-family:Georgia,Garamond,serif !important;word-break:break-all;word-break:break-word;-webkit-hyphens:auto;-moz-hyphens:auto;hyphens:auto;white-space:pre-wrap}#wrapper sup,#wrapper sub,#wrapper a.footnote{font-size:1.4ex;height:0;line-height:1;vertical-align:super;position:relative}#wrapper sub{vertical-align:sub;top:-1px}

</style>

</head>
<body class="normal">
  <div id="wrapper">
      <h1 id="backgroundstudy">Background Study</h1>

<p>Our method can be named view-dependent simulation level-of-detail (VSLOD) or view-dependent multiresolution simulation. </p>

<h2 id="1.simulation">1. Simulation</h2>

<h3 id="categoryofsimulationlevelofdetailslod">Category of Simulation level of detail (SLOD)</h3>

<p>Methods in this category change a dynamic model from complex one to simple one, for example from a full rigid body model to a point-mass model, to achieve faster simulation speed. </p>

<h4 id="carlsond.a.andhodginsj.k.1997.simulationlevelsofdetailforreal-timeanimation.graphicsinterface.1997.">Carlson, D.A. and Hodgins, J.K. 1997. Simulation levels of detail for real-time animation. Graphics Interface. (1997).</h4>

<p>This paper is the first SLOD paper I guess. They manually designed simplified dynamic models for particular system, in this case a legged creature. They used three levels of detail consisting of (1) rigid-body dynamic simulation, (2) point-mass model and kinematic model, and just (3) point-mass model only. </p>

<p>In (1), the system has three bodies (a hip and two parts of a telescoping leg) and four controlled degrees of freedom, such as pitch, roll and yaw of the body and the length of the leg. In (2) Hybrid kinematic/dynamic model, it &#8220;relies on a simple dynamic model for the motion of the body and computes the motion fo the leg kinematically&#8221;. Lastly, (3) is equivalent to (2) without the kinematic model, i.e. the system computes only the position of the body on the playing field using simple point-mass model. </p>

<p>They used two different criteria for selecting appropriate level of detail: The first one is about the possibility of dynamic interactions among objects. For example, if a creature is very close to a wall, it should be simulated dynamically (Figure 5). The second one is about view-dependency such as whether it is in the field of view and if so, how close it is to the viewer (Figure 6). </p>

<h4 id="brogand.c.andhodginsj.k.2002.simulationlevelofdetailformultiagentcontrol.newyorknewyorkusa2002199–206.">Brogan, D.C. and Hodgins, J.K. 2002. Simulation level of detail for multiagent control. (New York, New York, USA, 2002), 199–206.</h4>

<p>Summary goes here. </p>

<h4 id="chenneys.andforsythd.1997.view-dependentcullingofdynamicsystemsinvirtualenvironments.newyorknewyorkusa199755–58.">Chenney, S. and Forsyth, D. 1997. View-dependent culling of dynamic systems in virtual environments. (New York, New York, USA, 1997), 55–58.</h4>

<p>View-dependent culling is different than SLOD. They stop simulating a dynamic system once it goes out of view, and then use statistical sampling to predict the current state of the system when it comes back to the view. Thus, what they are doing is literally &#8220;culling dynamics&#8221;. They also utilize a human perception model to make the prediction more plausible and its errors less noticeable to human observer when the system goes in and out of the view (Consistency). </p>

<h4 id="obriend.etal.2001.automaticsimplificationofparticlesystemdynamics.2001210–257.">O&#8217;Brien, D. et al. 2001. Automatic simplification of particle system dynamics. (2001), 210–257.</h4>

<p>This paper introduced an approach that automatically generates approximated motion model in order to be used in SLOD but the method is limited to particle systems only. Manual generation of simplifed models is one of major shortcomings of SLOD. </p>

<h4 id="1.setasm.n.andgomesm.r.1995.dynamicsimulationofnaturalenvironmentsinvirtualreality.…invirtualenvironments.1995.">1. Setas, M.N. and Gomes, M.R. 1995. Dynamic simulation of natural environments in virtual reality. … in Virtual Environments. (1995).</h4>

<h4 id="2.perbetf.andcanim.-p.2001.animatingprairiesinreal-time.newyorknewyorkusa2001103–110.">2. Perbet, F. and Cani, M.-P. 2001. Animating prairies in real-time. (New York, New York, USA, 2001), 103–110.</h4>

<p>These two papers are very similar. </p>

<h4 id="beaudoinj.andkeyserj.2004.simulationlevelsofdetailforplantmotion.newyorknewyorkusa2004297–304.">Beaudoin, J. and Keyser, J. 2004. Simulation levels of detail for plant motion. (New York, New York, USA, 2004), 297–304.</h4>

<p>Summary goes here. </p>

<h4 id="bromc.etal.2007.simulationlevelofdetailforvirtualhumans.intelligentvirtualagents.springerberlinheidelberg.1–14.">Brom, C. et al. 2007. Simulation Level of Detail for Virtual Humans. Intelligent Virtual Agents. Springer Berlin Heidelberg. 1–14.</h4>

<p>Summary goes here. </p>

<h4 id="wardk.etal.2003.modelinghairusinglevel-of-detailrepresentations.200341–47.">Ward, K. et al. 2003. Modeling hair using level-of-detail representations. (2003), 41–47.</h4>

<p>Summary goes here. </p>

<h3 id="categoryofview-dependentfluidsimulation">Category of View-dependent fluid simulation</h3>

<p>Here are papers which add view-dependent criteria to conventional adaptive fluid simulation </p>

<h4 id="solenthalerb.andgrossm.2011.two-scaleparticlesimulation.siggraph11:siggraph2011papers.aug.2011.">Solenthaler, B. and Gross, M. 2011. Two-scale particle simulation. SIGGRAPH &#8217;11: SIGGRAPH 2011 papers. (Aug. 2011).</h4>

<p>They used simple camera information, such as the field of view, to assign high-resolution region to the part of fluid currently in the view frustum. </p>

<h4 id="barranb.a.2006.viewdependentfluiddynamics.2006.">Barran, B.A. 2006. View dependent fluid dynamics. (2006).</h4>

<p>Interestingly, the author used a cylindrical coordinate system for computational grid instead of a Cartesian coordinate system in order to make it adaptive to perspective projection. Due to the use of cylindrical coordinates, the system has various cell sizes, which becomes larger as it goes far away from the origin (Figure 2). </p>

<h4 id="1.bunlutangtumr.andkanongchaiyosp.2011.adaptivegridrefinementusingview-dependentoctreeforgrid-basedsmokesimulation.motioningames.springerberlinheidelberg.204–215.">1. Bunlutangtum, R. and Kanongchaiyos, P. 2011. Adaptive Grid Refinement Using View-Dependent Octree for Grid-Based Smoke Simulation. Motion in Games. Springer Berlin Heidelberg. 204–215.</h4>

<h4 id="2.bunlutangtumr.andkanongchaiyosp.2011.enhancedview-dependentadaptivegridrefinementforanimatingfluids.dec.2011.">2. Bunlutangtum, R. and Kanongchaiyos, P. 2011. Enhanced view-dependent adaptive grid refinement for animating fluids. (Dec. 2011).</h4>

<h4 id="3.kimj.etal.2006.view-dependentadaptiveanimationofliquids.etrijournal.2006.">3. Kim, J. et al. 2006. View-dependent adaptive animation of liquids. ETRI journal. (2006).</h4>

<p>Those three papers are very similar to one another. They used an octree in a Cartesian coordinate system for their computational grids, and then took into account camera information to refine their grids properly. </p>

<h4 id="yuegaoetal.2013.view-dependentmultiscalefluidsimulation.ieeetransactionsonvisualizationandcomputergraphics.192feb.2013178–188.">Yue Gao et al. 2013. View-Dependent Multiscale Fluid Simulation. IEEE Transactions on Visualization and Computer Graphics. 19, 2 (Feb. 2013), 178–188.</h4>

<p>Summary goes here. </p>

<h3 id="categoryofadpativefluidsimulationsnoview-dependency">Category of Adpative fluid simulations (No view-dependency)</h3>

<h4 id="losassof.etal.2004.simulatingwaterandsmokewithanoctreedatastructure.siggraph04:siggraph2004papers.aug.2004.">Losasso, F. et al. 2004. Simulating water and smoke with an octree data structure. SIGGRAPH &#8217;04: SIGGRAPH 2004 Papers. (Aug. 2004).</h4>

<p>This paper is one of the first adaptive fluid simulation papers (octree-based) so we might cite it when we mention those view-dependent fluid simulation papers. </p>

<h2 id="2.rendering">2. Rendering</h2>

<h3 id="categoryofgeometriclevel-of-detailglod">Category of Geometric level-of-detail (GLOD)</h3>

<p>Methods in this category switch to simplified model when the distance from camera is greater than thresholds. </p>

<h4 id="1.duchaineaum.etal.1997.roamingterrain:real-timeoptimallyadaptingmeshes.199781–88.">1. Duchaineau, M. et al. 1997. ROAMing terrain: Real-time Optimally Adapting Meshes. (1997), 81–88.</h4>

<h4 id="2.friedricha.etal.1999.smoothview-dependentrenderinginanimations.1999.">2. Friedrich, A. et al. 1999. Smooth View-Dependent Rendering in Animations. (1999).</h4>

<h4 id="3.hinsingerd.etal.2002.interactiveanimationofoceanwaves.newyorknewyorkusa2002161–166.">3. Hinsinger, D. et al. 2002. Interactive animation of ocean waves. (New York, New York, USA, 2002), 161–166.</h4>

<h4 id="4.hoppeh.1998.smoothview-dependentlevel-of-detailcontrolanditsapplicationtoterrainrendering.199835–42.">4. Hoppe, H. 1998. Smooth view-dependent level-of-detail control and its application to terrain rendering. (1998), 35–42.</h4>

<p>All the geometric level-of-detail papers are about rendering and I could find nothing specifically related to our project. </p>

<h3 id="categoryofview-dependentglodvglod">Category of View-dependent GLOD (VGLOD)</h3>

<p>Methods in this category use not only distance but also view frustum, occlusion, surface normal, screen-space projection and so on when choosing appropriate GLOD. They basically use view-dependency a lot, so there are a lot of criteria which seems useful to our project as well. </p>

<h4 id="xiaj.c.andvarshneya.1996.dynamicview-dependentsimplificationforpolygonalmodels.1996327–334.">Xia, J.C. and Varshney, A. 1996. Dynamic view-dependent simplification for polygonal models. (1996), 327–334.</h4>

<p>Section 3. Image-Space-Guided Simplification </p>

<ol>
<li>Local illumition:<br/>
Increase detail in a direction perpendicular to, and propotional to, the illumination gradient across the surface. It also uses surface normal.</li>
<li>Screen-space projection:<br/>
Use projected length instead of object-space length.</li>
<li>Visibility culling:<br/>
Remove back-facing polygons.</li>
<li>Silhouette boundaries:<br/>
Use projected length of silhouette edges to contorol smoothness of slihouette.</li>
</ol>

<h4 id="hoppeh.1997.view-dependentrefinementofprogressivemeshes.aug.1997.">Hoppe, H. 1997. View-dependent refinement of progressive meshes. (Aug. 1997).</h4>

<p>Section 4. Refinement Criteria </p>

<ol>
<li>View frustum</li>
<li>Surface orientation</li>
<li>Screen-space geometric error</li>
</ol>

<h4 id="funkhousert.a.andséquinc.h.1993.adaptivedisplayalgorithmforinteractiveframeratesduringvisualizationofcomplexvirtualenvironments.newyorknewyorkusa1993247–254.">Funkhouser, T.A. and Séquin, C.H. 1993. Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. (New York, New York, USA, 1993), 247–254.</h4>

<p>This paper is one of the most interesting papers in this list. To begin with, this paper is about view-dependent rendering to achieve faster rendering speed by selecting appropriate GLOD and rendering algorithms for each object, such as float, Gouraud and Phong shading. </p>

<p>In contrast to previous GLOD papers, The main point of this paper is that they didn&#8217;t set any static threshold for selecting LOD and rendering algorithm. Rather they made the system adaptively changes a threshold for selecting LOD to achieve uniform and bounded frame rate (Adaptive Detail Elision). In this process they rely on an optimization technique to counteract sudden changes of the complexity of scene due to a camera movement. </p>

<p>They mentioned two approaches for dynamic threshold: (1) Feedback and (2) Optimization. The first one adjusts the threshold based on the rendering time of the previous to meet the target frame rate. However, this method basically assumes that the complexity of scene is smoothly changing, which is not true if the camera moves quickly or jumps to a very different location. Thus, they opted for the second one, Optimization, which basically predicts the rendering time and the importance of each object in all possible configurations (every combination of LOD and rendering algorithm), and then renders objects in descending order of Value, which is a ratio of importance to rendering time, as long as the target frame rate allows. This is a greedy approximation because their original Optimization algorithm is NP-complete. </p>

<p>Section 5. Benefit Heuristic </p>

<ol>
<li>Semantics</li>
<li>Focus</li>
<li>Motion blur</li>
<li>Hysteresis</li>
</ol>

<h2 id="etcetera">Et cetera</h2>

<ul>
<li>Brooks, R.J. and Tobias, A.M. 1996. Choosing the best model: Level of detail, complexity, and model performance. Mathematical and Computer Modelling. 24, 4 (Aug. 1996), 1–14.</li>
<li>Debunne, G. et al. 2000. Adaptive simulation of soft bodies in real-time. (2000), 15–20.</li>
<li>Grzeszczuk, R. et al. 1998. NeuroAnimator. (New York, New York, USA, 1998), 9–20.</li>
<li>Ko, H. and Badler, N.I. 1992. Straight Line Walking Animation Based on Kinematic Generalization that Preserves the Original Characteristics. (1992).</li>
<li>Li, L. and Volkov, V. 2005. Cloth animation with adaptively refined meshes. Australian Computer Society, Inc.</li>
<li>Lindstrom, P. and Pascucci, V. 2002. Terrain simplification simplified: a general framework for view-dependent out-of-core visualization. IEEE Transactions on Visualization and Computer Graphics. 8, 3 (2002), 239–254.</li>
<li>Lindstrom, P. and Pascucci, V. 2001. Visualization of large terrains made easy. (2001), 363–574.</li>
<li>Lindstrom, P. et al. 1996. Real-time, continuous level of detail rendering of height fields. (New York, New York, USA, 1996), 109–118.</li>
<li>Liu, S.Q. et al. 2006. Real-time, dynamic level-of-detail management for three-axis NC milling simulation. Computer-Aided Design. 38, 4 (Apr. 2006), 378–391.</li>
<li>Luebke, D. and Erikson, C. 1997. View-dependent simplification of arbitrary polygonal environments. (New York, New York, USA, 1997), 199–208.</li>
<li>Müller, M. et al. 2003. Particle-based fluid simulation for interactive applications. (Jul. 2003).</li>
<li>Thomaszewski, B. et al. 2007. Advanced Topics in Virtual Garment Simulation Part 1. (Aug. 2007), 1–23.</li>
<li>Xia, J.C. et al. 1997. Adaptive real-time level-of-detail based rendering for polygonal models. IEEE Transactions on Visualization and Computer Graphics. 3, 2 (1997), 171–183.</li>
<li>Yan, J. 1985. Advances in Computer-Generated Imagery for Flight Simulation. IEEE Computer Graphics and Applications. 5, 8 (Aug. 1985), 37–51.</li>
</ul>

<h2 id="notrelated">Not related?</h2>

<ul>
<li>Popovic, Z. and Witkin, A. 1999. Physically based motion transformation. (New York, New York, USA, 1999), 11–20.</li>
</ul>
<!-- ##END MARKED WRAPPER## -->
    </div>
</body>
</html>